# -*- coding: utf-8 -*-
"""RandomForestRevisited.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/110unKptGEAXW8Bg_kXWMzEXe-i3X1NBs

# RANDOM FOREST REVISITED

# LLIBRERIES

Visualize trees libraries
"""

!apt-get install graphviz
!pip install graphviz

"""UCI MLR Respository"""

pip install ucimlrepo

"""LightGBM and Xgboost Libraries"""

!pip install lightgbm xgboost

"""Imports for other libraries"""

import numpy as np
import pandas as pd
import random
import pprint
import time
import os
from collections import Counter
from graphviz import Digraph
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

"""# DECISION TREE"""

class DecisionTree:
    """Implementacio de un arbre de decisio CART personalitzat que gestiona NUllS.

    Parametres
    ----------
    max_depth : int o None
        Profunditat m√†xima de l‚Äôarbre. Si √©s None, els nodes s‚Äôexpandeixen fins que totes les fulles siguin pures
        o continguin menys de `min_samples_split` mostres.
    min_samples_split : int, per defecte 2
        Nombre m√≠nim de mostres requerit per dividir un node intern.
    feature_subset : int o None
        Nombre de caracter√≠stiques a considerar a l‚Äôhora de buscar el millor punt de divisi√≥.
        Si √©s None, s‚Äôutilitzen totes les caracter√≠stiques.
    """
        #Constructor de l'arbre
    def __init__(self, max_depth=None, min_samples_split=2, feature_subset=None):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.feature_subset = feature_subset
        self.tree = None

    def fit(self, X: pd.DataFrame, y: pd.Series):
        """Entrena l'arbre de decisio"""
        self.n_features = self.feature_subset or X.shape[1] # Guarda num features per split
        self.y_train = y #Guarda y
        self.tree = self._build_tree(X, y, depth=0) #inici construccio arbre

    def _gini(self, y: pd.Series):
        """Calcula l'impuresa de Gini"""
        counts = y.value_counts().values
        probs = counts / counts.sum()  # Normalitzem per obtenir probabilitats
        return 1 - np.sum(probs ** 2)  # Retorna l'impuresa del node

    def _best_split(self, X: pd.DataFrame, y: pd.Series):
        """Troba la millor divisi√≥ segons el gain amb l'√≠ndex de Gini, gestionant valors nuls i categ√≤riques."""
        best_gain = 0
        best_feature = None
        best_threshold = None
        best_is_categorical = False

        current_gini = self._gini(y)  # Gini actual del node
        feature_idxs = random.sample(range(X.shape[1]), self.n_features)  # Selecci√≥ aleat√≤ria de caracter√≠stiques

        for feature in feature_idxs:
            col = X.iloc[:, feature]  # Agafa la columna corresponent a feature del conjunt

            # Detectem si √©s categ√≤rica
            is_categorical = col.dtype.name == "category" or col.dtype == object

            if is_categorical:
                #CATEG√íRIQUES
                values = col.dropna().unique()  # Guardem valors unics (sense NaNs)
                for val in values:
                    if pd.isna(val):  # Evitem usar NaN per fer split
                        continue
                    #Pertinen√ßa/grups per categ√≤riques i nulls apart
                    left_mask = col == val
                    right_mask = col != val
                    null_mask = col.isna()

                    #Si hi ha una branca pel llindar, salta al seguent
                    if left_mask.sum() == 0 or right_mask.sum() == 0:
                        continue

                    #Generem llistes de valors segons les mascares booleanes
                    y_left, y_right, y_null = y[left_mask], y[right_mask], y[null_mask]
                    #Calcul del Gini amb la feature i llindar actual de cada subconjunt
                    gini_left, gini_right, gini_null = self._gini(y_left), self._gini(y_right), self._gini(y_null)
                    #Calcul de l'imupresa mitjana ponderada segons la dimensio dels subconnjunts anteriors
                    weighted_gini = (
                        (len(y_left) * gini_left + len(y_right) * gini_right + len(y_null) * gini_null) / len(y)
                    )
                    #Calcul del gain global, per saber quant ha millorat el node
                    gain = current_gini - weighted_gini

                    #Historial que es sobreescriu per quedarnos al final amb el milllor gain i els parametres relacionats
                    if gain > best_gain:
                        best_gain = gain
                        best_feature = feature
                        best_threshold = val
                        best_is_categorical = True

            else:
                # NUM√àRIQUES
                thresholds = col.dropna().unique()  # Elimina NaNs per trobar valors reals
                for threshold in thresholds:
                    if pd.isna(threshold):  # Evitem thresholds nuls
                        continue

                    left_mask = col <= threshold
                    right_mask = col > threshold
                    null_mask = col.isna()

                    if left_mask.sum() == 0 or right_mask.sum() == 0:
                        continue

                    y_left, y_right, y_null = y[left_mask], y[right_mask], y[null_mask]
                    gini_left, gini_right, gini_null = self._gini(y_left), self._gini(y_right), self._gini(y_null)
                    weighted_gini = (
                        (len(y_left) * gini_left + len(y_right) * gini_right + len(y_null) * gini_null) / len(y)
                    )
                    gain = current_gini - weighted_gini

                    if gain > best_gain:
                        best_gain = gain
                        best_feature = feature
                        best_threshold = threshold
                        best_is_categorical = False

        # Si no es troba un gain millor a 0 o threshold √©s null, retorna None per indicar que no es pot construir m√©s ---> (fulla)
        if best_feature is None or pd.isna(best_threshold):
            return None, None, None
        else:
            return best_feature, best_threshold, best_is_categorical

    def _split_dataset(self, X: pd.DataFrame, y: pd.Series, feature, threshold, is_categorical):
        """Divideix el dataset en tres subconjunts: esquerra, dreta i valors nuls.
        Per categ√≤riques: esquerra = igualtat, dreta = diferent. Per num√®riques: esquerra = <= threshold, dreta = > threshold.
        """
        col = X.iloc[:, feature] #Accedim a la columna feature

        if is_categorical:
            left_mask = col == threshold     # Pertinen√ßa (categoria igual)
            right_mask = col != threshold    # Categoria diferent
        else:
            left_mask = col <= threshold     # Num√®rica segons threshold
            right_mask = col > threshold

        null_mask = col.isna()

        return (
            X[left_mask], y[left_mask],
            X[right_mask], y[right_mask],
            X[null_mask], y[null_mask]
        )

    def _build_tree(self, X: pd.DataFrame, y: pd.Series, depth):
        """Construcci√≥ recursiva de l'arbre, incloent una branca per valors nuls."""

        #Si s'ha de crear fulla es retorna la classe majoritaria
        #Raons: max depth, poques mostres per dividir o totes les clases son la mateixa
        if depth == self.max_depth or len(X) < self.min_samples_split or self._gini(y) == 0:
            return self._most_common_class(y)

        #Es busca el seguent millor split
        best_feature, best_threshold, is_categorical = self._best_split(X, y)

        #Si no s'ha trobat un split millor, retornem classe majoritaria, es crea fulla
        if best_feature is None or pd.isna(best_threshold):
            return self._most_common_class(y)

        #Es divideix el dataset segons el best_split trobat
        X_left, y_left, X_right, y_right, X_null, y_null = self._split_dataset(X, y, best_feature, best_threshold, is_categorical)

        #Es crea la branca especial peer nulls i es retorna fulla si no hi ha nulls a la branca, sino es continua recursivament
        if len(y_null) == 0:
            null_branch = self._most_common_class(y)
        else:
            null_branch = self._build_tree(X_null, y_null, depth + 1)

        #Return i crida rescursiva
        return {
            'feature': best_feature,
            'threshold': best_threshold,
            'is_categorical': is_categorical,
            'left': self._build_tree(X_left, y_left, depth + 1),
            'right': self._build_tree(X_right, y_right, depth + 1),
            'null': null_branch,
            'depth': depth
        }

    def _predict_one(self, row: pd.Series, node):
        """Passa una √∫nica mostra per l'arbre fins a trobar una fulla, manejant valors nuls i categ√≤rics."""
        if not isinstance(node, dict):
            return node  # Si √©s una fulla, retornem la predicci√≥

        #Rceuperem info del node
        feature = node["feature"]
        threshold = node["threshold"]
        is_categorical = node.get("is_categorical", False)

        value = row.iloc[feature]

        # Tractament de valors numerics com a strings ex:"5", Pandas els pot passar a strings a vegades, han de ser Float
        try:
            if not is_categorical:
                value = float(value)
        except (ValueError, TypeError):
            value = np.nan

        # Si √©s NaN, seguim la branca "null"
        if pd.isna(value):
            return self._predict_one(row, node["null"]) if "null" in node else self._most_common_class(self.y_train)

        # Comparaci√≥ segons tipus de variable
        if is_categorical:
            if value == threshold:
                return self._predict_one(row, node["left"])
            else:
                return self._predict_one(row, node["right"])
        else:
            if value <= threshold:
                return self._predict_one(row, node["left"])
            else:
                return self._predict_one(row, node["right"])

    def predict(self, X: pd.DataFrame):
        """Prediu la classe per a totes les mostres de X utilitzant un sol arbre de decisi√≥."""
        #Llista de prediccions
        predictions = np.array([self._predict_one(row, self.tree) for _, row in X.iterrows()])
        #Detecta si hi ha una prediccio buida i s'omple amb la classe mes comuna
        nan_mask = pd.isna(predictions)
        if np.any(nan_mask):
            predictions[nan_mask] = self._most_common_class(self.y_train)

        return predictions

    def _most_common_class(self, y):
        """Retorna la classe majoritaria"""
        #Elimina valors buits del target y
        y = y.dropna()
        #Troba el valor mes frequent(moda)
        mode_result = y.mode()
        return mode_result.iloc[0]

"""# RANDOM FOREST"""

class RandomForest:
    """
    Ensemble de DecisionTree CART amb suport per a dades num√®riques, categ√≤riques i valors nuls.

    Cada arbre s‚Äôentrena sobre una mostra bootstrap diferent i utilitza un
    subconjunt aleatori de caracter√≠stiques per a cada split.

    Parametres
    ----------
    n_trees : int
        Nombre d‚Äôarbres en el bosc.
    max_depth : Optional[int]
        Profunditat m√†xima per arbre. Si √©s None, no es limita per profunditat.
    max_features : Optional[int]
        Nombre de caracter√≠stiques seleccionades aleat√≤riament per split. Si √©s None,
        es consideren totes les caracter√≠stiques.
    min_samples_split : int, default=2
        Nombre m√≠nim de mostres requerit per fer un split.
    random_state : Optional[int]
        Contrasenya per al generador aleatori, per reproducibilitat.
    """

        #Constructor del bosc ---> Conjunt de DecisionTree's
    def __init__(self, n_trees=10, max_depth=None, min_samples_split=2, max_features=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.max_features = max_features
        self.trees = []

    def _bootstrap_sample(self, X: pd.DataFrame, y: pd.Series):
        """Crear una mostra bootstrap amb reempla√ßament (bagging), assegurant que es mant√© en format pandas"""
        #Obtenim nombre total de mostres
        n_samples = X.shape[0]
        #Generem llista de index aleatoris
        indices = np.random.choice(n_samples, size=n_samples, replace=True)
        #Retornem llista amb valors aleatoris del dataset original --> Bagging
        return X.iloc[indices], y.iloc[indices]

    def fit(self, X: pd.DataFrame, y: pd.Series):
        """Entrena el RF entrenant multiples arbres"""
        self.trees = []  # Reinicialitza la llista d'arbres en cada entrenament

        for _ in range(self.n_trees):
            #Crea amb bagging una mostra aleatoria pe cada arbre
            X_sample, y_sample = self._bootstrap_sample(X, y)
            #Crea el Decison Tree
            tree = DecisionTree(max_depth=self.max_depth,
                                min_samples_split=self.min_samples_split,
                                feature_subset=self.max_features)
            #Entrenem el Decison Tree
            tree.fit(X_sample, y_sample)
            #S'afageix el Decsion Tree al conjunt d'arbres(Random Forest) per participar en la predicci√≥
            self.trees.append(tree)

    def predict(self, X: pd.DataFrame):
        """Prediu la classe per a totes les mostres combinant les prediccions dels arbres."""

        #Recollim totes les prediccions de cada arbre com arrays 1D
        #Cada arbre retorna un array de prediccions, una per mostra
        #Tree_predictions sera una matriu de n_arbresXn_mostres
        tree_predictions = [
            np.ravel(tree.predict(X)) for tree in self.trees
        ]

        # Les posem en DataFrame transposat: files = mostres, columnes = arbres
        tree_predictions_df = pd.DataFrame(tree_predictions).T

        # Votaci√≥ per majoria, troba el valor mes frequent per fila
        final_predictions = tree_predictions_df.mode(axis=1)[0]

        #Retorna un pd.series amb les prediccions de totes les mostres
        return final_predictions

    def _majority_vote(self, predictions: pd.Series):
        """Retorna la classe mes comuna"""
        return predictions.mode()[0]  #usa `pandas.mode()`, m√©s eficient, es retorna un √∫nic valor, la prediccio final

"""# Functions

Graph Function
"""

from graphviz import Digraph
from pandas.api.types import is_categorical_dtype, is_object_dtype
import pandas as pd, numpy as np

def visualize_tree(tree, Xdata, parent=None, edge_label=None, graph=None):
    """Funci√≥ recursiva per dibuixar l'arbre, amb branches NULL i splits categ√≤rics."""
    if graph is None:
        graph = Digraph(format="png")

    if isinstance(tree, dict):
        feature_idx = tree['feature']
        col_name    = Xdata.columns[feature_idx]
        threshold   = tree['threshold']

        # 1) Detectem dtype real
        is_categorical = is_categorical_dtype(Xdata[col_name]) \
                      or is_object_dtype(Xdata[col_name])


        # 2) Formatem l‚Äôetiqueta del node
        thr_str   = f"{threshold:.2f}" if isinstance(threshold, float) else str(threshold)
        node_lbl  = f"{col_name}\nThreshold: {thr_str}"
        node_id   = str(id(tree))
        graph.node(node_id, node_lbl, style="filled", fillcolor="lightblue")
        if parent:
            graph.edge(parent, node_id, label=edge_label)

        # 3) Triarem els s√≠mbols segons el tipus
        left_lbl  = "==" if is_categorical else "‚â§"
        right_lbl = "!=" if is_categorical else ">"

        # 4) Crides recursives (sempre passant Xdata!)
        visualize_tree(tree['left'],  Xdata, node_id, left_lbl,  graph)
        visualize_tree(tree['right'], Xdata, node_id, right_lbl, graph)
        if 'null' in tree:
            visualize_tree(tree['null'], Xdata, node_id, "NULL", graph)

    else:
        # Bloc fulla: extraiem un valor escalar
        if isinstance(tree, pd.Series):
            leaf_val = tree.iloc[0]
        elif isinstance(tree, (list, np.ndarray, tuple)):
            leaf_val = tree[0]
        else:
            leaf_val = tree

        node_lbl = f"Leaf: {leaf_val:.2f}"
        node_id  = str(id(tree))
        graph.node(node_id, node_lbl, shape="box", style="filled", fillcolor="lightgreen")
        if parent:
            graph.edge(parent, node_id, label=edge_label)

    return graph

"""Funcio per trobar parametres optims"""

def optimal_params(Xdata):
    """ Toba els valors optims per entrenar segons les dimensions d'un dataset"""
    n_files = Xdata.shape[0]
    n_features = Xdata.shape[1]

    print(f" Dataset: {n_files} files, {n_features} columnes")

    if n_files <= 1000:
        print("\n Recomanaci√≥ per a *dataset petit* (‚â§ 1000 files):")
        print("  - n_trees: 10‚Äì50")
        print("  - max_depth: 5‚Äì10")
        print("  - min_samples_split: 2‚Äì5")
        print(f"  - max_features: 1‚Äì3 o sqrt(n) = {int(np.sqrt(n_features))}")
    else:
        print("\n Recomanaci√≥ per a *dataset gran* (> 10.000 files):")
        print("  - n_trees: 100‚Äì200")
        print("  - max_depth: 10‚Äì20")
        print("  - min_samples_split: 10‚Äì20")
        print(f"  - max_features: sqrt(n) = {int(np.sqrt(n_features))}")

"""# DATASETS"""

from ucimlrepo import fetch_ucirepo

# LUNG CANCER DATASET
# https://archive.ics.uci.edu/dataset/62/lung+cancer

lung_cancer = fetch_ucirepo(id=62)

# data (as pandas dataframes)
Xdata = lung_cancer.data.features
ydata_full = lung_cancer.data.targets

from ucimlrepo import fetch_ucirepo

# MYOCARDIAL INFARCTION
# https://archive.ics.uci.edu/dataset/579/myocardial+infarction+complications

myocardial_infarction_complications = fetch_ucirepo(id=579)

# data (as pandas dataframes)
Xdata = myocardial_infarction_complications.data.features
ydata_full = myocardial_infarction_complications.data.targets

from ucimlrepo import fetch_ucirepo

# HEART DISEASE
# https://archive.ics.uci.edu/dataset/45/heart+disease

heart_disease = fetch_ucirepo(id=45)

# data (as pandas dataframes)
Xdata = heart_disease.data.features
ydata_full = heart_disease.data.targets

from ucimlrepo import fetch_ucirepo

# BREAST CANCER WISCONSIN
# https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original

breast_cancer_wisconsin_original = fetch_ucirepo(id=15)

# data (as pandas dataframes)
Xdata = breast_cancer_wisconsin_original.data.features
ydata_full = breast_cancer_wisconsin_original.data.targets

from ucimlrepo import fetch_ucirepo

# MAMOGRAPGHIC MASS
# https://archive.ics.uci.edu/dataset/161/mammographic+mass

mammographic_mass = fetch_ucirepo(id=161)

# data (as pandas dataframes)
Xdata = mammographic_mass.data.features
ydata_full= mammographic_mass.data.targets

from ucimlrepo import fetch_ucirepo

#CHRONIC KIDNEY DISEASE
# https://archive.ics.uci.edu/dataset/336/chronic+kidney+disease

chronic_kidney_disease = fetch_ucirepo(id=336)

# data (as pandas dataframes)
Xdata = chronic_kidney_disease.data.features
ydata_full = chronic_kidney_disease.data.targets

"""# ANALISIS DEL DATASET

Analitzem Mostra de dades
"""

print("Mostra de dades:")
print(Xdata.head())

# Dimensions del dataset
print(f"\n Dimensions: {Xdata.shape[0]} files, {Xdata.shape[1]} columnes")

# Valors √∫nics (primeres 5 columnes com a exemple)
print("\n Valors √∫nics (primeres 5 columnes com a exemple):")
for col in Xdata.columns[:5]:
    print(f"{col}: {Xdata[col].unique()}")

# An√†lisi de valors nuls
print("\n Columnes amb valors nuls (NaN):")
nulls = Xdata.isnull().sum()
nulls_percent = (nulls / len(Xdata)) * 100
for col in Xdata.columns:
    if nulls[col] > 0:
        print(f"{col}: {nulls[col]} nuls ({nulls_percent[col]:.2f}%)")

# An√†lisi de valors sospitosos com -1 o "?"
print("\n Valors sospitosos (-1, '?') per columna:")
for col in Xdata.columns:
    suspicious = ((Xdata[col] == -1) | (Xdata[col] == "?")).sum()
    if suspicious > 0:
        print(f"{col}: {suspicious} valors sospitosos")

# Tipus de variables
num_vars = Xdata.select_dtypes(include=['int64', 'float64']).shape[1]
cat_vars = Xdata.select_dtypes(include=['object', 'category']).shape[1]
print(f"\n Tipus de variables: {num_vars} num√®riques, {cat_vars} categ√≤riques")

#Parametres optims per fer el RF
optimal_params(Xdata)

"""Analitzem Dades Target"""

#Posibles targets
print(ydata_full.head())

#Triar Target si hi ha mes de un com a Myocardial Infarction Complications (multilabel)
ydata = ydata_full["Class"]
print(ydata.head())

ydata = ydata_full #Si nomes hi ha un target

print(ydata["Class"].unique()) #Mirar posibles valors del target

#Proporcio ydata, per saber el balanceig i afegir oversampling si cal
proporcions = ydata["Class"].value_counts(normalize=True) * 100
print(proporcions.round(2))  # Mostrem amb 2 decimals

"""Oversampling complet (codi per copiar i portar al Main Test)"""

from sklearn.utils import shuffle

# Trobar mida de la classe majorit√†ria
max_count = y_train.value_counts().max()

# Inicialitzar llistes per concatenar
X_balanced = []
y_balanced = []

# Iterar per cada classe i aplicar oversampling si cal
for cls in np.unique(y_train):
    X_cls = X_train[y_train == cls]
    y_cls = y_train[y_train == cls]

    # Oversample nom√©s si cal (classe minorit√†ria)
    if len(X_cls) < max_count:
        X_resampled = X_cls.sample(n=max_count, replace=True, random_state=42)
        y_resampled = y_cls.sample(n=max_count, replace=True, random_state=42)
    else:
        X_resampled = X_cls
        y_resampled = y_cls

    X_balanced.append(X_resampled)
    y_balanced.append(y_resampled)

# Concatenar i barrejar
X_oversampled = pd.concat(X_balanced, ignore_index=True)
y_oversampled = pd.concat(y_balanced, ignore_index=True)
X_oversampled, y_oversampled = shuffle(X_oversampled, y_oversampled, random_state=42)

"""Undersampling complet (codi per copiar i portar al Main Test)"""

from sklearn.utils import shuffle

# Encontrar el tama√±o de la clase minoritaria (la m√°s peque√±a)
min_count = y_train.value_counts().min()

# Inicializamos listas para guardar subconjuntos
X_balanced = []
y_balanced = []

# Iterar por cada clase y aplicar undersampling si cal
for cls in np.unique(y_train):
    X_cls = X_train[y_train == cls]
    y_cls = y_train[y_train == cls]

    # Subsample (recorte) a min_count
    X_resampled = X_cls.sample(n=min_count, random_state=42)
    y_resampled = y_cls.sample(n=min_count, random_state=42)

    X_balanced.append(X_resampled)
    y_balanced.append(y_resampled)

# Concatenar y barajar
X_undersampled = pd.concat(X_balanced, ignore_index=True)
y_undersampled = pd.concat(y_balanced, ignore_index=True)
X_undersampled, y_undersampled = shuffle(X_undersampled, y_undersampled, random_state=42)

"""# MAIN TEST"""

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report
)

# Substitueix valors especials per NaN
Xdata.replace(['?', -1, 'unknown', 'Unknown', ''], np.nan, inplace=True)

# Divisi√≥ en train i test
#Amb stratify mantenim prorporcio de la clase en test i train
X_train, X_test, y_train, y_test = train_test_split(Xdata, ydata, test_size=0.2, random_state=42, stratify=ydata)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)
# // ----- SAMPLING -----

#Afegir aqui oversampling o undersampling

# ----- SAMPLING -----

# Entrenar el model amb el nou RandomForest adaptat
rf_custom = RandomForest(n_trees=25, max_depth=5, max_features=2)
rf_custom.fit(X_train, y_train)

# Predir amb el Random Forest personalitzat
start_time = time.time()
y_pred_custom = rf_custom.predict(X_test)
time_custom = time.time() - start_time

# Validaci√≥ de resultats
print("\n Predicci√≥ del model:")
print(f"Tipus: {type(y_pred_custom)}")
print(f"Forma: {y_pred_custom.shape}")
print(f"Primers valors: {y_pred_custom[:10].values if isinstance(y_pred_custom, pd.Series) else y_pred_custom[:10]}")
print(f" Temps de predicci√≥: {time_custom:.4f} s")

# Avaluaci√≥ amb m√∫ltiples m√®triques
print("\n M√®triques de classificaci√≥:")

print(f"Accuracy: {accuracy_score(y_test, y_pred_custom):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_custom, average='macro'):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_custom, average='macro'):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_custom, average='macro'):.4f}")

print("\n Matriu de confusi√≥:")
print(confusion_matrix(y_test, y_pred_custom))

# Resum complet per classe
print("\n Informe de classificaci√≥:")
print(classification_report(y_test, y_pred_custom))

"""Visualitzacio d'un arbre d'entrenament"""

treeView = rf_custom.trees[11].tree
graph = visualize_tree(treeView, X_train)

# Guardem a drive
folder_path = '/content/drive/MyDrive/TFG/Trees'
# path
filepath = os.path.join(folder_path, 'tree11')
# Guarda PNG
graph.render(filename=filepath, format='png', cleanup=True)

graph

"""# DATASET IMPUTAT AMB Fillna()"""

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report
)

#  Substitueix valors especials per NaN
Xdata.replace(['?', -1, 'unknown', 'Unknown', ''], np.nan, inplace=True)

#  Imputaci√≥ de valors nuls amb la mitjana de cada columna num√®rica
Xdata_imputed = Xdata.copy()
for col in Xdata_imputed.columns:
    if Xdata_imputed[col].dtype in ['float64', 'int64']:
        Xdata_imputed[col] = Xdata_imputed[col].fillna(Xdata_imputed[col].mean())

#  Divisi√≥ en train i test amb dades imputades
X_train, X_test, y_train, y_test = train_test_split(Xdata_imputed, ydata, test_size=0.2, random_state=42, stratify=ydata)

print("üîç X_test shape:", X_test.shape)
print("üîç y_test shape:", y_test.shape)


#  Entrenar el model amb Random Forest personalitzat
rf_custom = RandomForest(n_trees=20, max_depth=5, max_features=5)
rf_custom.fit(X_train, y_train)

#  Predir amb el Random Forest personalitzat
start_time = time.time()

y_pred_custom = rf_custom.predict(X_test)
time_custom = time.time() - start_time

#  Validaci√≥ de resultats
print("\n Predicci√≥ del model:")
print(f"Tipus: {type(y_pred_custom)}")
print(f"Forma: {y_pred_custom.shape}")
print(f"Primers valors: {y_pred_custom[:10].values if isinstance(y_pred_custom, pd.Series) else y_pred_custom[:10]}")
print(f" Temps de predicci√≥: {time_custom:.4f} s")

#  Avaluaci√≥ amb m√∫ltiples m√®triques
print("\n M√®triques de classificaci√≥:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_custom):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_custom, average='macro'):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_custom, average='macro'):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_custom, average='macro'):.4f}")

print("\n Matriu de confusi√≥:")
print(confusion_matrix(y_test, y_pred_custom))

print("\n Informe de classificaci√≥:")
print(classification_report(y_test, y_pred_custom))

"""# CROSS VALIDATION"""

from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Configuraci√≥ de cross-validation
k = 10  # pots provar amb 5 o 10
kf = KFold(n_splits=k, shuffle=True, random_state=42)

# Guardem m√®triques de cada fold
accuracies = []
precisions = []
recalls = []
f1s = []

for fold, (train_index, val_index) in enumerate(kf.split(Xdata)):
    print(f"\nFold {fold + 1}/{k}")

    X_train, X_val = Xdata.iloc[train_index], Xdata.iloc[val_index]
    y_train, y_val = ydata.iloc[train_index], ydata.iloc[val_index]

    # oversampling aqui

    rf_custom = RandomForest(n_trees=20, max_depth=5, max_features=5)
    rf_custom.fit(X_train, y_train)

    y_pred = rf_custom.predict(X_val)

    # M√®triques
    accuracies.append(accuracy_score(y_val, y_pred))
    precisions.append(precision_score(y_val, y_pred, average='macro'))
    recalls.append(recall_score(y_val, y_pred, average='macro'))
    f1s.append(f1_score(y_val, y_pred, average='macro'))

# Resultats mitjans
print("\n Mitjanes Cross-Validation:")
print(f"Accuracy: {np.mean(accuracies):.4f}")
print(f"Precision (macro): {np.mean(precisions):.4f}")
print(f"Recall (macro): {np.mean(recalls):.4f}")
print(f"F1 Score (macro): {np.mean(f1s):.4f}")

"""# XGBoost Test"""

import xgboost as xgb

# Substitueix valors especials per NaN
Xdata.replace(['?', -1, 'unknown', 'Unknown', ''], np.nan, inplace=True)
ydata = ydata.replace({2: 0, 4: 1}) #xgboost espera 0,1 ---> Pel Breast Cancer Wisconsin
# Divisi√≥ en train i test
#Amb stratify mantenim prorporcio de la clase en test i train
X_train, X_test, y_train, y_test = train_test_split(Xdata, ydata, test_size=0.2, random_state=42, stratify=ydata)
print(" X_test shape:", X_test.shape)
print(" y_test shape:", y_test.shape)

#  Entrenar el model amb XGBoost
from xgboost import XGBClassifier

xgb_clf = XGBClassifier(
    n_estimators=20,
    max_depth=5,
    learning_rate=0.1,
    eval_metric='mlogloss',
    random_state=42
)
xgb_clf.fit(X_train, y_train)

#  Predir amb XGBoost
start_time = time.time()
y_pred_xgb = xgb_clf.predict(X_test)
time_xgb = time.time() - start_time

# Validaci√≥ de resultats
print("\n Predicci√≥ amb XGBoost:")
print(f"Tipus: {type(y_pred_xgb)}")
print(f"Forma: {y_pred_xgb.shape}")
print(f"Primers valors: {y_pred_xgb[:10]}")
print(f" Temps de predicci√≥: {time_xgb:.4f} s")

#  Avaluaci√≥ amb m√∫ltiples m√®triques
print("\n M√®triques de classificaci√≥:")

print(f"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_xgb, average='macro'):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_xgb, average='macro'):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_xgb, average='macro'):.4f}")

print("\n Matriu de confusi√≥:")
print(confusion_matrix(y_test, y_pred_xgb))

#  Resum complet per classe
print("\n Informe de classificaci√≥:")
print(classification_report(y_test, y_pred_xgb))

"""# LightGBM Test"""

import lightgbm as lgb

# Substitueix valors especials per NaN
Xdata.replace(['?', -1, 'unknown', 'Unknown', ''], np.nan, inplace=True)

# Divisi√≥ en train i test
#Amb stratify mantenim prorporcio de la clase en test i train
X_train, X_test, y_train, y_test = train_test_split(Xdata, ydata, test_size=0.2, random_state=42, stratify=ydata)
print("üîç X_test shape:", X_test.shape)
print("üîç y_test shape:", y_test.shape)


# Entrenar el model amb LightGBM
lgbm_clf = lgb.LGBMClassifier(
    n_estimators=20,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)

lgbm_clf.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='logloss',
    callbacks=[lgb.early_stopping(stopping_rounds=10)]
)

# Predir amb LightGBM
start_time = time.time()
y_pred_lgbm = lgbm_clf.predict(X_test)
time_lgbm = time.time() - start_time

# Validaci√≥ de resultats
print("\n Predicci√≥ amb LightGBM:")
print(f"Tipus: {type(y_pred_lgbm)}")
print(f"Forma: {y_pred_lgbm.shape}")
print(f"Primers valors: {y_pred_lgbm[:10]}")
print(f" Temps de predicci√≥: {time_lgbm:.4f} s")

# Avaluaci√≥ amb m√∫ltiples m√®triques
print("\n M√®triques de classificaci√≥:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_lgbm):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_lgbm, average='macro'):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_lgbm, average='macro'):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred_lgbm, average='macro'):.4f}")

print("\n Matriu de confusi√≥:")
print(confusion_matrix(y_test, y_pred_lgbm))

# Resum complet per classe
print("\n Informe de classificaci√≥:")
print(classification_report(y_test, y_pred_lgbm))